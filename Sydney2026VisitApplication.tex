\documentclass{amsart}
\usepackage{mathtools,upref,siunitx,upquote,fancyvrb,xspace,color}
\usepackage[hyphens]{url}
\usepackage[utf8]{inputenc}
\usepackage{esdiff}

\input{FJHDef.tex}


\usepackage{algpseudocode}
\usepackage{algorithm, algorithmicx}
\algnewcommand\algorithmicparam{\textbf{Parameters:}}
\algnewcommand\PARAM{\item[\algorithmicparam]}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\RETURN{\State \textbf{Return }}

\usepackage[a4paper, margin=1in]{geometry}

\begin{document}
\title{Proposed Research for Visit Sponsored by the \\
Sydney Mathematical Research Institute in 2026 \\
\bigskip 
\large{Rigorous Adaptive Algorithms for Problems with Randomness}}
\author{Fred J. Hickernell, Illinois Institute of Technology}

\maketitle

The proposed research at University of New South Wales (UNSW) and the University of Sydney builds upon two of my research themes:
\begin{itemize}
    \item Quasi-Monte Carlo methods for solving problems with randomness, and
    \item Rigorous adaptive algorithms for problems defined on cones of inputs.
\end{itemize}
By collaborating with the Sydney mathematical community, I hope to take advantage of our mutual strengths to develop rigorous and efficient, adaptive algorithms where none yet exist.

\section{Quasi-Monte (QMC) Carlo Methods }
QMC methods approximate the population mean, $\mu=\Ex(Y) = \int_{[0,1]^d} f(\vx) \, \dif \vx$---where the random variable $Y = f(\vX)$ is defined by $\vX \sim \cu[0,1]^d$---by the sample mean, $\hmu_n := n^{-1} \sum_{i=1}^n f(\vx_i)$, but using low discrepancy (LD) sequences $\vx_1, \vx_2, \ldots$ rather than independent and identically distributed (IID) sequences \cite{DicEtal22a,DicEtal14a,DicPil10a}.

\begin{figure}[h]
\centering
\includegraphics[width = 0.27\textwidth]{Figures/Uniform.pdf} \qquad 
\includegraphics[width = 0.27\textwidth]{Figures/Lattice.pdf}
    \caption{Comparison of an IID sequence with a popular LD sequence.  Blue dots represent the first $16$ points, the orange dots the next $16$ points, the green dots the next $32$ points, and so on.  The lattice LD points are more evenly distributed than the IID points, which have clusters and gaps. \label{fig:points}}
\end{figure}

Figure \ref{fig:points} shows how LD sequences fill space more evenly than IID sequences.  The \emph{discrepancy}, $D\bigl(\{x_i\}_{i=1}^n\bigr)$,  \cite{Hic97a,Hic99a} is a quantitative measure of how close the empirical distribution of $\{x_i\}_{i=1}^n$ is to the uniform distribution.  Moreover, the error of QMC, $\lvert \mu - \hmu_n\rvert$, is bounded above by $D\bigl(\{x_i\}_{i=1}^n\bigr) \, \lVert f\rVert$, where  $\lVert f\rVert$ is a semi-norm of $f$, often called the \emph{variation}.  Different choices of the seminorm imply different definitions of the discrepancy.  LD sequences have small discrepancy.

The UNSW group has pioneered many innovations in the theory and application of QMC.

\section{Rigorous Adaptive Algorithms}
Although the discrepancy of a point set, $D\bigl(\{x_i\}_{i=1}^n\bigr)$ may often be computed, computing $\lVert f\rVert$ is harder than the original problem of computing $\mu=\int_{[0,1]^d} f(\vx) \, \dif \vx$.  Thus the error bound above is not suitable as a stopping criterion for determining the $n$ required to ensure that $\abs{\mu - \hat{\mu}_n}$ does not exceed a given error tolerance, $\varepsilon$.  Sometimes practitioners will use random replications of the point set, $\{x_i\}_{i=1}^n$, and use the standard deviation of the resulting random sample of $\hat{\mu}_n$ to estimate the error.

Hickernell and collaborators track the discrete Fourier (complex exponential) coefficients of the function data, $\{f(\vx_i)\}_{i=1}^n$ for lattice LD sequences $\{x_i\}_{i=1}^n$.  Under certain assumptions on the general decay of the true Fourier coefficients of $f$, one can construct theoretically justified stopping criteria for choosing $n$ to guarantee that $\abs{\mu - \hat{\mu}_n} \le \varepsilon$ \cite{JimHic16a}.  An analogous theory holds for digital sequences \cite{HicJim16a}.

Alternatively, one can assume that $f$ is an instance of a Gaussian process and construct credible intervals for $\mu$, which can be used to determine the sample size, $n$, required to meet the desired error tolerance.  In general, this approach requires expensive $\Order(n^3)$ matrix operations, but when suitable covariance kernels for the Gaussian process are matched with the LD lattice or digital net sequence, the computational burden reduces  to a very feasible $\Order(n \log(n))$ \cite{RatHic19a,JagHic22a}.

Hickernell and collaborators have also developed rigorous adaptive algorithms for IID Monte Carlo computation of $\mu$ \cite{HicEtal14a} and for univariate integration, approximation, and optimization \cite{ChoEtal17a,HicETal14b}.
%Software libraries in Matlab and Python that implement many of these adaptive algorithms have been developed by Hickernell and collaborators \cite{ChoEtal21a,QMCPy2020a}.  
The core assumption in all of these rigorous adaptive algorithms, is that $f$ belongs to a \emph{cone} of functions that are not too peaky.

\section{Proposed Activities During the Visit}

During the proposed visit to Sydney, Hickernell will give a series of several lectures on adaptive algorithms and QMC and the role of cones in constructing adaptive algorithms.  The focus will be to summarize what is known and identify several accessible research problems.  This includes the following:
\begin{itemize}
    \item The adaptive algorithms for QMC apply to a fixed dimension, $d$. Many interesting applications involve very large or even infinite $d$, which benefit from multilevel methods \cite{Gil14a}.  We will attempt to develop rigorous stopping criteria for multilevel methods, especially those that arise in the uncertainty quantification problems that Frances Kuo and her collaborators are experts in.

    \item The adaptive univariate integration algorithm seems ripe to be expanded to finite element methods with their a posteriori error bounds in one or more dimensions.  Many interesting applications involve partial differential equations with random coefficients.  We will explore these possibilities for constructing rigorous, adaptive algorithms.

    \item The lectures given by Hickernell may expose algorithms that the audience is developing that are amenable to the approach that Hickernell has used for developing adaptive stopping rules.  We will explore these possibilities.
\end{itemize}
The visit to University of Sydney will be July 20 -- August 2, 2026, with Professor Georg Gottwald as host.  The visit to University of New South Wales will be August 3--16, 2026, with Professor Frances Kuo as host.  %Follow up research after the visit will be carried out remotely or when we meet at professional conferences.

\bibliographystyle{amsplain}
\bibliography{FJH25,FJHown23}

\end{document}